# 단어 빈도수 API 최적화

## 문제 상황

현재 단어 빈도수 API를 호출했을 떄  
이 요청의 처리 과정은 다음과 같다.

1. 최근 한달 내의 articles에 있는 모든 단어를 가져온다.
2. 가져온 단어들을 형태소 분석한다.
3. 형태소들을 빈도수로 계산한다.
4. 빈도수가 높은 순으로 정렬하여 반환한다.

### 문제점 1 - 한달 내의 모든 articles 가져오기

단어 빈도수 API는 최근 한달 내의 articles에 있는 단어들을 가져오는 API이다.

이때 항상 모든 articles을 가져오기 때문에  
데이터가 많아질수록 처리 시간이 길어진다.

그리고 어떤 유저가 요청하든 항상 같은 결과를 반환한다.

그리고 만약 오늘 하루에 articles 관련 쓰기 작업이 없을 경우,  
오늘 하루 동안은 단어 빈도수 API가 항상 같은 결과를 반환한다.

-> 한달 내의 articles을 가져오는 것은 비효율적이다.

### 문제점 2 - 형태소 분석 라이브러리 사용

현재 형태소 분석을 위해 사용하는 라이브러리는 `KOMORAN`이다.

이 라이브러리를 사용하면서 OOM(Out Of Memory)이 발생했다.  
(그것도 스프링을 처음 실행했을 때)

위를 해결하기 위해 좀 더 가벼운 버전으로 변경했지만,  
여전히 메모리 사용량이 많이 나타났다.

## 해결 방안

### 1 - 형태소 분석 처리

우리 서버의 상황을 고려했을 때 최적의 형태소 분석기를 찾는다.

1. <u>형태소 분석을 위한 라이브러리 사용</u>  
   장점: 네트워크를 사용하지 않기 때문에 빠르다.  
   단점: 형태소 분석을 라이브러리에서 직접 처리하기 때문에 리소스를 많이 사용한다.
2. <u>외부 API 호출</u>  
   장점: 형태소 분석을 외부 API에서 처리하기 때문에 리소스를 사용하지 않는다.  
   단점: 네트워크를 사용하기 때문에 느릴 수 있다.

위 두가지를 따졌을 때  
현재 상황에서는 외부 API를 호출하는 것이 더 효율적일 것으로 판단된다.

> 현재 우리 서버는 리소스가 부족하기 때문에  
> (vCPU: 1, RAM: 1GB)

### 2 - 외부 API 종류 및 선택

형태소 분석을 위한 외부 API는 다음과 같다.

1. ETRI Open API
   - 사용량: 하루 5000건 제한 & 단일 요청 최대 10000자 제한
2. Google Natural Language AI
   - 300$까지 무료 & 그 이후에는 요금 발생
3. 한글을 지원하는 챗봇 API
   - 사용량: 경우에 따라 다르다

위 세 가지 API 중  
1번 ETRI Open API를 사용하는 것으로 결정했다.

<u>이유 1 - 형태소 분석의 정확도</u>  
1,2 번은 형태소 분석을 위한 API이지만 3번은 범용성이 높은 챗봇 API이기 때문에,  
형태소 분석의 정확도를 고려했을 때 3번은 적합하지 않다고 판단했다.

<u>이유 2 - 비용</u>  
2번은 무료 사용량이 300$까지이기 때문에,  
이후에 요금이 발생할 수 있다.  
따라서 비용을 고려했을 때 1번을 선택했다.

### 3 - 단어 빈도수 데이터 캐싱

어떤 유저가 요청하던, 단어 빈도수 API는 항상 같은 결과를 반환한다.  
(오늘 하루에 articles 관련 쓰기 작업이 없을 경우)

따라서 단어 빈도수 데이터를 캐싱하는 것이 효율적이다.

---

외부 API는 일일 최대 5000건까지 사용 가능하다.  
그리고 한번의 요청에 최대 10000자까지 분석할 수 있다.

이를 고려하여 형태소를 분석해야 한다.

---

## 데이터 저장 방법

### 방법 1 - 모든 데이터를 분석하여 저장한다

데이터 타입  
(단어id, 형태소, 빈도수) - (BIGINT, VARCHAR(255), BIGINT)

방법  
모든 데이터를 분석하여 저장한다.

장점
1. (2번 방법과 비교했을 때)  
   31일전 & 1일전 데이터를 변경하는 작업이 필요없다.

단점
1. 데이터가 많아질수록 저장 공간이 많이 필요하다.
2. 이 데이터는 최근 한달 내의 데이터만 필요하기 때문에,  
   오래된 데이터는 사용되지 않는다.



### 방법 2 - 일별 데이터를 분석하여 저장한다

데이터 타입  
(단어id, 형태소, 빈도수, 날짜) - (BIGINT, VARCHAR(255), BIGINT, DATE)

방법  
매일 데이터를 분석하여 저장한다.  
(날짜로 인덱싱)

장점
1. 한달 내의 데이터만 저장하기 때문에  
   저장 공간을 절약할 수 있다.

단점
1. 매일 31일전 데이터를 삭제 & 1일전 데이터를 추가하는 작업이 필요하다.
2. spendDate의 값이 변경되면, 데이터를 추가하는 작업이 필요하다.

> 원래 쓰기 작업 발생하면,  
> 두 방법 모두 변경될 수 있지만,  
> 2번 방법은 날짜 변경에도 데이터를 추가하는 작업이 필요하다.

---

결국 두 방법의 차이점은

1번 방법은 모든 데이터를 저장하는 것  
2번 방법은 최근 한달 내의 데이터만 저장하는 것

1번은 31일 전 & 1일 전 데이터를 변경하는 작업이 필요없지만  
저장 공간이 많이 필요하다.

2번은 저장 공간을 절약할 수 있지만  
매일 31일 전 데이터를 삭제 & 1일 전 데이터를 추가하는 작업이 필요하다.

---

### 결론

2번 방법을 약간 변형하여 사용하기로 결정했다.

1일전 데이터를 추가하는 작업은 그대로 유지하되,  
31일 전 데이터를 삭제하는 작업은 수행하지 않기로 했다.

`이유`
1. 모든 데이터에 대한 정보를 따로 저장하는 것은 무리라고 판단했다.
2. 31일 전 데이터를 삭제하는 작업은 비효율적이라고 판단했다.  
   (너무 많이 누적되면 예전 데이터를 삭제하는 것은 추후에 고려하기로 했다)
3. (핵심) 1번이든 2번이든 1일전 데이터를 추가하는 작업은 필요하기 때문이다.

---

## 쓰기 작업 시나리오

쓰기 작업이 발생할 때마다  
형태소 분석을 위해 외부 API를 호출하는 것은 비효율적이다.  
(일일 제한이 있기 때문)

따라서 쓰기 작업이 발생하면 이를 지연시켜  
최대 10000자까지 모였을 때 외부 API를 호출하는 것으로 결정했다.

> 이때 새로 생성되는 데이터와 삭제되는 데이터를 구분하여  
> 따로 관리해야 한다.

