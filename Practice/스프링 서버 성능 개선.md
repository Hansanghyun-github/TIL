0-1 아키텍쳐

EC2: t2.micro
(vCPU: 1, 메모리: 1 GiB)

자바 & 스프링으로 서버 구성 

RDS: db.t3.micro  
(vCPU: 2, 메모리: 1 GiB)

MySQL로 DBMS 구성

---

0-2 부하테스트 시나리오

vuser 100명

OAuth2 인증을 받은 vuser가  
20분동안 `/api/articles/main API`를 호출한다.

0-3 DB 데이터

약 80만개의 article 데이터가 있다.  
(500명의 유저가, 최근 2년동안 매일 2개씩 일기를 작성했다고 가정)

main API 조회시, 총 한 달 동안 한 유저가 작성한 일기들을 반환한다.  
(약 60~88개)

---

1 맨처음 부하테스트 수행

TPS = 30

visualVM으로 확인한 결과  
CPU 사용량 - 97%, 그 중 GC가 차지하는 비율 - 20%~30%

cloudwatch를 통해 확인한 DB 서버의 CPU 사용량 - 약 20%

병목은 스프링 서버에서 발생하고 있었다.

1. 스프링 서버의 병목 원인 분석

top 같은 명령어들로 스레드별 CPU 사용량 분석 결과  
`VM Thread`와 `C2 CompilerThread0` 스레드의 CPU 사용량이 높게 나타났다.  
(이 스레드들은 GC 스레드, JIT 컴파일러 스레드)  
-> 어쩔 수 없이 사용해야 하는 스레드

그 외 스레드들은 모두 1%~2% 점유율을 보였다.  
-> 특정 스레드가 병목을 발생시키는 것이 아니라, 단순히 부하가 많아 CPU 사용량이 높게 나온 것

2. GC 점유율 높은 이유 분석

만약 메모리 누수가 발생했다면 GC 이후 계속 힙 점유율이 높아져야 하는데,  
힙 점유율은 현재 힙 사용량 밑을 유지했다.  
(원활하게 GC가 일어난다는 뜻)

즉 단순히 Garbage Object가 많아서 발생한 문제로 파악된다.

// todo - 테스트 서버 DB 데이터 -> 로컬 DB로 복제

3. DB 커넥션을 얻는데 시간이 너무 오래 걸린다.

현재 DB Connection Pool과 ThreadPool의 사이즈는 default 값으로 설정되어 있다.

DB Connection Pool size = 10  
ThreadPool size = 10(min), 200(max)

> max threads는 200으로 설정되어 있지만,  
> 부하가 발생했을 때 최대 110개의 스레드만 생성되었다.

스레드 풀 사이즈에 비해 커넥션 풀 사이즈가 작아서,  
커넥션을 얻는데 시간이 오래 걸린다.

이는 스레드 풀 & 커넥션 풀 사이즈를 조절해야 한다.

---

발생한 문제

1. CPU 사용량이 높다. -> 단순히 부하가 많아서 발생한 문제
2. CPU 사용량 중, GC 점유율이 높다. -> Garbage Object가 많아서 발생한 문제 (DB 복제로 확인 필요)
3. DB 커넥션을 얻는데 시간이 오래 걸린다. -> 스레드 풀 사이즈에 비해 커넥션 풀 사이즈가 작아서 발생한 문제

> DB CPU 점유율이 스프링 서버보다 낮다.  
> 이때 DB 스레드별 state 확인 필요 -> AWS RDS

> GC를 줄이려면 최소힙을 늘려야 좋을듯,  
> -> 더 나아가서 최대힙과 최소힙을 같은 값으로 설정하는 것도 고려
> 
> 왜냐하면 이 서버 말고 다른 프로그램이 돌아가는 서버가 없기 때문에,  
> 최대힙을 늘려도 다른 프로그램에 영향을 미치지 않을 것이다.
> 
> (최대힙과 최소힙을 같게 하면 유연성이 떨어지지만, 우리 서버에서는 큰 문제가 없을 것이다)  
> (리소스 경쟁이 없기 때문)

---

2 성능 개선

> 적용하기 간단한 순서대로 적용

1. 커넥션 수 줄이기

맨 처음 세션인증을 수행할 때 DB에 조회를 하는데,  
이로 인해 불필요한 커넥션이 소모된다.  
실제 비즈니스 로직에서도 해당 세션의 유효성을 검증하기 떄문에  
세션 인증 단계에서는 DB 조회는 하지 않는 것으로 결정  

2. user.getArticles() 제거

위 로직을 수행하면, 특정 유저의 모든 article 들을 조회한다.  
(날짜에 관계없이)

현재 API에서 원하는건 이번 달에 해당하는 유저의 article만 조회하면 되기 때문에,  
날짜에 의한 필터링 작업을 DB에서 해오는게 좋을 것 같다.

3. DB 인덱스 활용

현재 article을 조회하는 쿼리를 인덱스를 이용하기 위해

(user_id, spend_date) 인덱스를 생성하고,  
인덱스를 타기 위한 쿼리로 로직 변경

> (spend_date, user_id) 인덱스는 비효율적  
> (첫번째 칼럼이 날짜라서 두번째 칼럼으로 필터링 안될 확률 높음)

4. 톰캣 스레드 풀의 maxThreads 조절

전체 그래프 출력 & 30개로 조정

5. 비즈니스 로직 개선

쿼리 조회 2개 -> 1개로 줄여서 테스트

결과 그래프로 출력

> 쿼리 개수를 줄이는 작업을 마지막에 한 이유  
> 
> 

---

스레드풀 & 커넥션풀 사이즈 조정

커넥션풀 추천 공식  
`connections = ((core_count * 2) + effective_spindle_count)`

[About Pool Sizing](https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing)

스레드풀 추천 공식  
`Number of threads = Number of Available Cores * (1 + Wait time / Service time)`

[How to set an ideal thread pool size](https://engineering.zalando.com/posts/2019/04/how-to-set-an-ideal-thread-pool-size.html)

일단 위 공식을 참고하여,  
위 결과를 최소숫자로 정하고

점점 늘리면서 성능을 확인해보자.

> 이때 스레드 풀의 최대/최소 사이즈는 나중에 생각하자.  
> (테스트 할 떄는 같게 설정)
> 
> 어차피 부하가 많아지면 최대 사이즈로 설정될 것이다.

---

### 스레드 풀 & 커넥션 풀 테스트 시나리오

스레드 풀 사이즈는 그냥 작은것 부터 큰것까지 전부 테스트 해본다.  
현재 SQL time이 나중에도 같을 것이라고 확신하지 않기 떄문에, (1)  
그리고 main API 말고 다른 API도 많기 때문에, (2)  
이 API 만 가지고 확신하기 어렵다.

-> 모든 경우를 테스트 해보고 결과를 확인하자.

커넥션 풀은 위 공식을 참고한다.  
vCPU가 2개이고, HDD가 아닌 SSD를 사용하므로,  
effective_spindle_count는 0으로 둔다.  
-> connections = 4

> 일단 테스트니까 effective_spindle_count가 1인것도 테스트 해보자.

그리고 일단 hikariCP의 기본값인 10도 같이 테스트 해보자.  

(커넥션 풀 사이즈 = 4, 5, 10)

스프링 서버의 vCPU = 1  
모든 API가 DB나 외부 API를 호출하기 때문에  
스레드 풀 사이즈는 vCPU의 2배(=2)로 시작한다.

2로 시작해서 몇까지 테스트 해볼 것인가?  

> 톰캣의 default 설정은 10,200이다.  
> 이 설정을 가지고 부하를 주었을 때, 실행된 스레드의 최대 개수는  
> 100를 조금 넘었다.  
> (< 110)
> 
> 즉 현재 부하로는 생성되는 스레드가 110개 이하이다.
> 
> 2부터 110까지 로그 스케일로 테스트 해보자.

(스레드 풀 사이즈 = 2, 3, 4, 5, 7, 10, 20, 30, 50, 70, 100)

최소힙(-Xms)과 최대힙(-Xmx)을 같은 값으로 설정해야 하나?

현재 컴퓨터의 메모리 용량은 1GiB  
스프링 서버를 올리기 전의 메모리 사용량은 31.7%  
(약 300MiB)

최대힙은 512MiB로 설정한다.  
최대힙만큼 사용해도 전체 메모리는 약 80% 정도 사용한다.  

> 힙 영역을 제외한  
> JVM의 오버헤드, 스택, 메타스페이스, 코드 캐시 등을 고려해도  
> 512MiB로 설정해도 충분할 것이다.

최소힙은 256MiB로 설정한다.  
(최대힙의 절반)

> 최대 힙은 진짜 메모리 요구량을 고려한 값이다.  
> 대부분 컴퓨터의 적정 메모리 사용량은 80% 이하이다.  
> 따라서 최소힙은 최대힙의 절반으로 설정한다.

---

### 테스트 스펙

스레드 풀 사이즈 = 2, 3, 4, 5, 7, 10, 20, 30, 50, 70, 100  
(최대/최소 사이즈는 같게 설정)  
커넥션 풀 사이즈 = 4, 5, 10  
최소힙 = 256MiB  
최대힙 = 512MiB

그 외 설정은 모두 default 값으로 설정한다.

---

## 테스트 결과

(Connection Pool size = 4 일 때)

|maxThreads|TPS| CPU utilization| memory(MB - free) | DB CPU |
|--|--|--|-|--|
|1|145|36.9|590,96|17|
|2|250|61.3|540,100|26.3|
|3|289|70|576,81|30|
|4|311|78|560,76|31.5|
|5|312|81|570,87|31.9|
|7|311|79.4|559,81|31.9|
|10|308|81.8|550,71|31.6|
|20|304|78.9|569,71|30.9|
|30|298|82.3|560,70|30.6|
|50|274|81.9|573,132|28|
|70|268|83|576,75|27.6|
|100|267|85|600,80|27.4|

여기서 스프링 서버의 CPU utilization이나 메모리 상황은  
특정 범위 안에서 변동이 있다.  
그래서 이 메트릭들로는 유의미한 결과를 얻을 수 없었다.

이 결과 중 유의미한 결과가 있는 부분은  
TPS와 DB CPU 메트릭이다.

maxThreads가 4 ~ 7일 때 TPS가 가장 높았다.  
(311 ~ 312)

그리고 DB CPU는 31.5 ~ 31.9로 거의 비슷했다.

maxThreads가 4보다 작을 대 DB CPU 메트릭은 감소했다.  
이는 maxThreads가 1에서 4로 증가하면서  
병렬화가 더 많이 일어나서 DB CPU가 증가했음을 의미한다.

> 싱글 코어 CPU에서 실행된다고 해도,  
> 특정 스레드가 DB에 쿼리를 날리는 동안  
> 다른 스레드가 다른 쿼리를 날릴 수 있기 때문에  
> 병렬화가 일어나서 DB CPU가 증가하고 TPS가 증가한다.

그리고 maxThreads가 10 이상일 때 TPS, DB CPU 메트릭이 감소했다.

> 이는 스레드 풀 사이즈가 커지면 커질수록  
> 스케쥴링 오버헤드가 커지기 때문이다.

// todo 컨텍스트 스위칭 메트릭 비교

---

### 현재 테스트의 한계점

하지만 이 결과에는 한가지 문제가 있다.  
-> maxThreads가 4 이상일 떄 CPU utilization이 80% 이상이다.

일단 서버의 적정 CPU utilization은 80% 이하이다.  

> 그리고 위와 같은 부하테스트를 계속 진행하면서  
> AWS EC2의 CPU credit이 고갈되어 CPU 성능이 떨어진 적이 있었다.

CPU의 적정 사용량과 CPU credit이 고갈된 것을 보고,  
스프링 서버의 CPU utilization을 80% 이하로 제한해야 한다고 판단했다.

---

## CPU limit 이후 테스트 결과

cgroup을 이용해 스프링 서버의 CPU limit을 70%로 설정하고,  
다시 테스트를 진행했다.

> 이때 유의미한 결과를 얻기 위해 maxThreads는 3 ~ 10으로 설정했다.
> (CPU utilization이 70%까지 도달하는 size부터 테스트)  
> (10 이상을 측정하지 않은 이유는, 위 결과를 통해 의미가 없다고 판단했기 때문)

(Connection Pool size = 4 일 때)

|maxThreads| TPS | CPU utilization  | DB CPU |
|--|-----|------------------|--------|
|3| 260 | 68.3             | 27     |
|4| 270 | 70               | 27.3   |
|5| 265 | 70               | 26.6   |
|7| 263 | 70               | 26.7   |
|10| 264 | 70               |26.2|

// todo 결과 분석

// todo CP size 5 일때도 테스트

